# ğŸ¤– IA-Personal para ADead-BIB

**Sistema de IA Personal Ultra-Ligero**

> Tu asistente personal que aprende de ti, recuerda tus conversaciones y se integra con ADead-BIB.

---

## ğŸ‡µğŸ‡ª Made with â¤ï¸ in Peru

**Author:** Eddi AndreÃ© Salazar Matos  
**Email:** eddi.salazar.dev@gmail.com  
**Version:** 1.0.0

---

## âœ¨ CaracterÃ­sticas

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| **Memoria Persistente** | Recuerda conversaciones entre sesiones |
| **Contexto Personal** | Aprende tu nombre, intereses y preferencias |
| **Aprendizaje Continuo** | Mejora con cada interacciÃ³n |
| **Ultra-Ligero** | Solo ~0.5 MB de RAM |
| **100% Privado** | Todo se procesa localmente |
| **IntegraciÃ³n ADead-BIB** | Operaciones aceleradas sin runtime |
| **IntegraciÃ³n Ollama** | LLM local para respuestas avanzadas |

---

## ğŸš€ Inicio RÃ¡pido

### Desde la carpeta IA_Personal
```powershell
cd IA_Personal
python -m IA_Personal
```

### Modos de EjecuciÃ³n
```powershell
# Chat estÃ¡ndar
python -m IA_Personal

# Modo turbo (mÃ¡s rÃ¡pido)
python -m IA_Personal --turbo

# ğŸ® Con aceleraciÃ³n GPU (CUDA) - NUEVO!
python -m IA_Personal --gpu

# Con aceleraciÃ³n ADead-BIB
python -m IA_Personal --adead

# Con Ollama LLM
python -m IA_Personal --ollama

# Todas las integraciones (GPU + Ollama)
python -m IA_Personal --full

# Demo del sistema
python -m IA_Personal --demo

# Benchmark de rendimiento
python -m IA_Personal --benchmark

# Benchmark GPU especÃ­fico
python -m IA_Personal --gpu --benchmark

# InformaciÃ³n del sistema
python -m IA_Personal --info
```

---

## ğŸ“ Estructura del Proyecto

```
IA_Personal/
â”œâ”€â”€ __init__.py              # MÃ³dulo principal
â”œâ”€â”€ __main__.py              # Punto de entrada CLI
â”œâ”€â”€ README.md                # Esta documentaciÃ³n
â”‚
â”œâ”€â”€ core/                    # NÃºcleo del sistema
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ia_personal.py       # Sistema principal
â”‚   â”œâ”€â”€ memory.py            # Memoria persistente
â”‚   â”œâ”€â”€ context.py           # Contexto personal
â”‚   â”œâ”€â”€ tokenizer.py         # Tokenizador inteligente
â”‚   â””â”€â”€ model.py             # Transformer ligero
â”‚
â”œâ”€â”€ integrations/            # Integraciones externas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adead_accelerator.py # AceleraciÃ³n ADead-BIB
â”‚   â””â”€â”€ ollama_chat.py       # IntegraciÃ³n Ollama LLM
â”‚
â”œâ”€â”€ ui/                      # Interfaces de usuario
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chat.py              # Chat interactivo
â”‚   â””â”€â”€ cli.py               # LÃ­nea de comandos
â”‚
â””â”€â”€ data/                    # Datos persistentes (auto-generado)
    â”œâ”€â”€ memories.json        # Memorias guardadas
    â”œâ”€â”€ profile.json         # Perfil del usuario
    â”œâ”€â”€ adead_cache/         # Cache de binarios
    â””â”€â”€ exports/             # Conversaciones exportadas
```

---

## ğŸ’¬ Comandos de Chat

### Comandos Especiales
| Comando | DescripciÃ³n |
|---------|-------------|
| `/ayuda` | Muestra ayuda |
| `/memoria` | EstadÃ­sticas de memoria |
| `/perfil` | Tu perfil personal |
| `/buscar [texto]` | Busca en memorias |
| `/exportar` | Exporta la conversaciÃ³n |
| `/stats` | EstadÃ­sticas del sistema |
| `/ollama` | Info de Ollama |
| `/limpiar` | Limpia la pantalla |
| `/salir` | Termina el chat |

### Frases de Aprendizaje
| Frase | AcciÃ³n |
|-------|--------|
| "Me llamo [nombre]" | Aprende tu nombre |
| "Me gusta [algo]" | Aprende tus intereses |
| "Recuerda que [algo]" | Guarda informaciÃ³n |

---

## ğŸ”§ Uso ProgramÃ¡tico

### Uso BÃ¡sico
```python
from IA_Personal import IAPersonal

ia = IAPersonal()
response = ia.chat("Hola, me llamo Carlos")
print(response)
```

### Con AceleraciÃ³n ADead-BIB
```python
from IA_Personal import IAPersonalADead

ia = IAPersonalADead()
ia.chat("Hola")
ia.benchmark_acceleration()
```

### Con Ollama LLM
```python
from IA_Personal import IAPersonalOllama

ia = IAPersonalOllama(ollama_model="tinyllama")
response = ia.chat("Explica quÃ© es la inteligencia artificial")
print(response)
```

### ConfiguraciÃ³n Personalizada
```python
from IA_Personal import IAPersonal, IAPersonalConfig

config = IAPersonalConfig(
    vocab_size=15000,
    embed_dim=128,
    num_layers=2,
    temperature=0.7,
    max_memory_items=1000,
)

ia = IAPersonal(config)
```

---

## ğŸ“Š Rendimiento

| MÃ©trica | Valor |
|---------|-------|
| **RAM Total** | ~0.5-1.3 MB |
| **Vocabulario** | 289+ tokens |
| **Tiempo de Respuesta** | <50 ms |
| **Agregar 100 memorias** | ~100 ms |
| **Buscar 100 veces** | ~11 ms |

### ğŸ® Rendimiento GPU (RTX 3060 12GB)

| OperaciÃ³n | CPU (ms) | GPU (ms) | Speedup |
|-----------|----------|----------|---------|
| MatMul 256x256 | 0.50 | 0.45 | 1.1x |
| MatMul 512x512 | 2.30 | 1.48 | 1.6x |
| MatMul 1024x1024 | 10.51 | 5.19 | **2.0x** |
| Attention 128x64 | 0.48 | 0.37 | 1.3x |
| Attention 256x128 | 0.85 | 0.46 | 1.9x |
| Attention 512x256 | 4.12 | 0.93 | **4.4x** |

**Speedup promedio: 2.0x** con GPU activada

---

## ğŸ¦™ IntegraciÃ³n Ollama

Para respuestas avanzadas usando LLMs locales:

### InstalaciÃ³n
```powershell
# Instalar Ollama
winget install Ollama.Ollama

# Iniciar servidor
ollama serve

# Descargar modelo
ollama pull tinyllama
```

### Uso
```powershell
python -m IA_Personal --ollama
```

Las preguntas complejas (explica, quÃ© es, escribe, etc.) se responden automÃ¡ticamente con Ollama.

---

## âš¡ IntegraciÃ³n ADead-BIB

AceleraciÃ³n de operaciones matemÃ¡ticas usando binarios nativos:

```powershell
python -m IA_Personal --adead
```

Operaciones aceleradas:
- Softmax
- GELU
- Layer Normalization
- MultiplicaciÃ³n de matrices

---

## ğŸ”’ Privacidad

- **100% Local**: Todos los datos en tu mÃ¡quina
- **Sin Internet**: No requiere conexiÃ³n
- **Sin TelemetrÃ­a**: No se envÃ­an datos
- **Datos Tuyos**: Puedes ver/editar/eliminar

---

## ğŸ“ Licencia

Apache 2.0 - Libre para uso personal y comercial.

---

**Â¡Disfruta tu IA Personal!** ğŸš€
