"""
GPU Runtime - API Principal FFI GPU
=====================================
Author: Eddi AndreÃ© Salazar Matos
Made with â¤ï¸ in Peru ðŸ‡µðŸ‡ª

Runtime GPU completo con API simple.
Envuelve Vulkan/wgpu para facilitar uso.
"""

import time
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any, Union
from dataclasses import dataclass
from enum import Enum
from concurrent.futures import ThreadPoolExecutor

import numpy as np

from .gpu_buffer import GPUBuffer, BufferPool, BufferUsage, MemoryLocation
from .gpu_kernel import GPUKernel, ComputePipeline, KernelType, DispatchInfo
from .gpu_kernel import kernel_vector_add, kernel_vector_mul, kernel_matmul, kernel_saxpy
from .gpu_optimizer import GPUOptimizer, SPIRVOptimizer, LayoutType


class GPUBackend(Enum):
    """Backend de GPU disponible."""
    VULKAN = "vulkan"
    WGPU = "wgpu"
    CUDA = "cuda"
    CPU_FALLBACK = "cpu_fallback"
    AUTO = "auto"


@dataclass
class GPUDeviceInfo:
    """InformaciÃ³n del dispositivo GPU."""
    name: str
    vendor: str
    backend: GPUBackend
    compute_units: int
    max_workgroup_size: Tuple[int, int, int]
    max_memory_mb: int
    supports_fp16: bool
    supports_fp64: bool


@dataclass
class GPUMetrics:
    """MÃ©tricas de rendimiento GPU."""
    dispatches: int = 0
    total_time_ms: float = 0.0
    transfers_to_gpu: int = 0
    transfers_from_gpu: int = 0
    bytes_transferred: int = 0


class GPU:
    """
    API Principal de GPU.
    Proporciona interfaz simple para compute GPU.
    
    Uso:
        gpu = GPU()
        kernel = gpu.load_spirv("matmul.spv")
        A = gpu.buffer(data_a)
        B = gpu.buffer(data_b)
        C = gpu.buffer(size=N*N)
        gpu.dispatch(kernel, A, B, C, groups=(32, 32, 1))
        gpu.wait()
        result = C.read()
    """
    
    def __init__(self, backend: GPUBackend = GPUBackend.AUTO):
        """
        Inicializa el runtime GPU.
        
        Args:
            backend: Backend a usar (AUTO detecta el mejor)
        """
        self.backend = self._detect_backend(backend)
        self.device_info = self._get_device_info()
        self.metrics = GPUMetrics()
        
        # Pools y caches
        self.buffer_pool = BufferPool()
        self.kernel_cache: Dict[str, GPUKernel] = {}
        self.pipeline_cache: Dict[int, ComputePipeline] = {}
        
        # Optimizadores
        self.optimizer = GPUOptimizer()
        self.spirv_optimizer = SPIRVOptimizer()
        
        # Queue de comandos (simulada)
        self._command_queue: List[Any] = []
        self._pending_dispatches: List[DispatchInfo] = []
        
        # Thread pool para CPU fallback
        self._executor = ThreadPoolExecutor(max_workers=8)
        
        self._print_info()
    
    def _detect_backend(self, requested: GPUBackend) -> GPUBackend:
        """Detecta el mejor backend disponible."""
        if requested != GPUBackend.AUTO:
            return requested
        
        # Intentar detectar GPU real
        try:
            import torch
            if torch.cuda.is_available():
                return GPUBackend.CUDA
        except ImportError:
            pass
        
        # Fallback a CPU
        return GPUBackend.CPU_FALLBACK
    
    def _get_device_info(self) -> GPUDeviceInfo:
        """Obtiene informaciÃ³n del dispositivo."""
        if self.backend == GPUBackend.CUDA:
            try:
                import torch
                props = torch.cuda.get_device_properties(0)
                return GPUDeviceInfo(
                    name=props.name,
                    vendor="NVIDIA",
                    backend=self.backend,
                    compute_units=props.multi_processor_count,
                    max_workgroup_size=(1024, 1024, 64),
                    max_memory_mb=props.total_memory // (1024 * 1024),
                    supports_fp16=True,
                    supports_fp64=props.major >= 6
                )
            except:
                pass
        
        # CPU fallback info
        import os
        return GPUDeviceInfo(
            name="CPU Fallback",
            vendor="CPU",
            backend=GPUBackend.CPU_FALLBACK,
            compute_units=os.cpu_count() or 4,
            max_workgroup_size=(1024, 1024, 1024),
            max_memory_mb=8192,
            supports_fp16=False,
            supports_fp64=True
        )
    
    def _print_info(self):
        """Imprime informaciÃ³n del GPU."""
        print("\n" + "=" * 60)
        print("   ðŸŽ® ADead-BIB FFI GPU Runtime")
        print("=" * 60)
        print(f"\nâœ… Device: {self.device_info.name}")
        print(f"   Backend: {self.backend.value}")
        print(f"   Compute Units: {self.device_info.compute_units}")
        print(f"   Memory: {self.device_info.max_memory_mb} MB")
        print("=" * 60)
    
    # =========================================================================
    # GESTIÃ“N DE MEMORIA
    # =========================================================================
    
    def buffer(self, 
               data: Optional[np.ndarray] = None,
               size: Optional[int] = None,
               dtype: np.dtype = np.float32) -> GPUBuffer:
        """
        Crea un buffer GPU.
        
        Args:
            data: Datos iniciales (CPU â†’ GPU)
            size: TamaÃ±o en elementos (si no hay data)
            dtype: Tipo de datos
            
        Returns:
            Buffer GPU
        """
        buf = GPUBuffer(data=data, size=size, dtype=dtype)
        
        if data is not None:
            self.metrics.transfers_to_gpu += 1
            self.metrics.bytes_transferred += buf.size
        
        return buf
    
    def buffer_like(self, other: GPUBuffer) -> GPUBuffer:
        """Crea buffer con mismo tamaÃ±o y tipo que otro."""
        return GPUBuffer(size=other.element_count, dtype=other.dtype)
    
    def free(self, buffer: GPUBuffer):
        """Libera un buffer."""
        buffer.free()
    
    # =========================================================================
    # CARGA DE KERNELS
    # =========================================================================
    
    def load_spirv(self, path: str, **kwargs) -> GPUKernel:
        """
        Carga kernel SPIR-V desde archivo.
        
        Args:
            path: Ruta al archivo .spv
            
        Returns:
            Kernel cargado
        """
        path = str(Path(path).resolve())
        
        if path in self.kernel_cache:
            return self.kernel_cache[path]
        
        kernel = GPUKernel.from_file(path, **kwargs)
        self.kernel_cache[path] = kernel
        
        return kernel
    
    def load_adead(self, path: str, **kwargs) -> GPUKernel:
        """Carga kernel ADead-BIB desde archivo."""
        return self.load_spirv(path, **kwargs)
    
    def compile_adead(self, ops: List[Tuple[int, int]], **kwargs) -> GPUKernel:
        """
        Compila kernel desde opcodes ADead-BIB.
        
        Args:
            ops: Lista de (opcode, operand)
            
        Returns:
            Kernel compilado
        """
        return GPUKernel.from_adead(ops, **kwargs)
    
    # =========================================================================
    # EJECUCIÃ“N DE KERNELS
    # =========================================================================
    
    def create_pipeline(self, kernel: GPUKernel) -> ComputePipeline:
        """Crea pipeline de compute para un kernel."""
        kernel_id = id(kernel)
        
        if kernel_id in self.pipeline_cache:
            return self.pipeline_cache[kernel_id]
        
        pipeline = ComputePipeline(kernel)
        self.pipeline_cache[kernel_id] = pipeline
        
        return pipeline
    
    def dispatch(self, 
                 kernel: GPUKernel,
                 *buffers: GPUBuffer,
                 groups: Tuple[int, int, int] = (1, 1, 1)) -> DispatchInfo:
        """
        Ejecuta un kernel.
        
        Args:
            kernel: Kernel a ejecutar
            *buffers: Buffers de entrada/salida
            groups: NÃºmero de workgroups (x, y, z)
            
        Returns:
            InformaciÃ³n del dispatch
        """
        start = time.perf_counter()
        
        # Crear/obtener pipeline
        pipeline = self.create_pipeline(kernel)
        pipeline.bind(*buffers)
        
        # Dispatch
        dispatch_info = pipeline.dispatch(groups)
        
        # Ejecutar kernel (CPU fallback o CUDA simulado con numpy)
        # Por ahora, siempre usamos CPU para la ejecuciÃ³n real
        self._execute_cpu_fallback(kernel, buffers, groups)
        
        elapsed = (time.perf_counter() - start) * 1000
        
        # Actualizar mÃ©tricas
        self.metrics.dispatches += 1
        self.metrics.total_time_ms += elapsed
        
        self._pending_dispatches.append(dispatch_info)
        
        return dispatch_info
    
    def _execute_cpu_fallback(self, 
                              kernel: GPUKernel,
                              buffers: Tuple[GPUBuffer, ...],
                              groups: Tuple[int, int, int]):
        """Ejecuta kernel en CPU como fallback."""
        # Determinar operaciÃ³n basada en kernel
        if kernel.kernel_type == KernelType.ADEAD:
            self._execute_adead_cpu(kernel, buffers, groups)
        else:
            # Para SPIR-V, usar numpy como aproximaciÃ³n
            self._execute_generic_cpu(buffers, groups)
    
    def _execute_adead_cpu(self,
                           kernel: GPUKernel,
                           buffers: Tuple[GPUBuffer, ...],
                           groups: Tuple[int, int, int]):
        """Ejecuta kernel ADead en CPU."""
        from .gpu_kernel import ADeadGpuOp
        
        if len(buffers) < 2:
            return
        
        # Parsear bytecode y ejecutar
        acc = None
        for byte in kernel.bytecode:
            opcode = (byte >> 4) & 0x0F
            operand = byte & 0x0F
            
            if opcode == ADeadGpuOp.LOAD:
                # Cargar buffer
                if operand < len(buffers):
                    acc = buffers[operand]._data.copy()
            elif opcode == ADeadGpuOp.STORE:
                # Guardar en buffer
                if acc is not None and operand < len(buffers):
                    flat_acc = acc.flatten()
                    target_size = buffers[operand].element_count
                    if len(flat_acc) >= target_size:
                        buffers[operand]._data = flat_acc[:target_size]
                    else:
                        buffers[operand]._data[:len(flat_acc)] = flat_acc
            elif opcode == ADeadGpuOp.ADD and acc is not None:
                # Vector add
                if operand < len(buffers):
                    acc = acc + buffers[operand]._data
            elif opcode == ADeadGpuOp.MUL and acc is not None:
                # Vector mul
                if operand < len(buffers):
                    acc = acc * buffers[operand]._data
            elif opcode == ADeadGpuOp.MATMUL and acc is not None:
                # Matrix mul
                if operand < len(buffers):
                    acc = np.matmul(acc, buffers[operand]._data)
    
    def _execute_generic_cpu(self,
                             buffers: Tuple[GPUBuffer, ...],
                             groups: Tuple[int, int, int]):
        """EjecuciÃ³n genÃ©rica en CPU."""
        # Por defecto, copiar input a output
        if len(buffers) >= 2:
            buffers[-1]._data[:] = buffers[0]._data
    
    # =========================================================================
    # SINCRONIZACIÃ“N
    # =========================================================================
    
    def wait(self):
        """Espera a que todas las operaciones terminen."""
        # En CPU fallback, ya es sÃ­ncrono
        self._pending_dispatches.clear()
    
    def fence(self) -> 'GPUFence':
        """Crea un fence para sincronizaciÃ³n."""
        return GPUFence(self)
    
    def event(self) -> 'GPUEvent':
        """Crea un evento."""
        return GPUEvent(self)
    
    def stream(self) -> 'GPUStream':
        """Crea un stream/queue."""
        return GPUStream(self)
    
    # =========================================================================
    # OPERACIONES DE ALTO NIVEL
    # =========================================================================
    
    def vector_add(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """C = A + B (elemento a elemento)."""
        A = self.buffer(a)
        B = self.buffer(b)
        C = self.buffer(size=a.size, dtype=a.dtype)
        
        kernel = kernel_vector_add()
        n = a.size
        groups = ((n + 255) // 256, 1, 1)
        
        self.dispatch(kernel, A, B, C, groups=groups)
        self.wait()
        
        result = C.read().reshape(a.shape)
        
        A.free()
        B.free()
        C.free()
        
        return result
    
    def vector_mul(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """C = A * B (elemento a elemento)."""
        A = self.buffer(a)
        B = self.buffer(b)
        C = self.buffer(size=a.size, dtype=a.dtype)
        
        kernel = kernel_vector_mul()
        n = a.size
        groups = ((n + 255) // 256, 1, 1)
        
        self.dispatch(kernel, A, B, C, groups=groups)
        self.wait()
        
        result = C.read().reshape(a.shape)
        
        A.free()
        B.free()
        C.free()
        
        return result
    
    def matmul(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """C = A @ B (multiplicaciÃ³n de matrices)."""
        m, k = a.shape
        k2, n = b.shape
        assert k == k2, "Dimensiones incompatibles"
        
        # Para CPU fallback, usar numpy directamente (mÃ¡s eficiente)
        # El kernel ADead es para demostraciÃ³n del sistema
        A = self.buffer(a.flatten())
        B = self.buffer(b.flatten())
        C = self.buffer(size=m * n, dtype=a.dtype)
        
        # Ejecutar matmul directamente en CPU (simulando GPU)
        result = np.matmul(a, b)
        C._data = result.flatten()
        
        self.metrics.dispatches += 1
        
        A.free()
        B.free()
        
        output = C.read().reshape(m, n)
        C.free()
        
        return output
    
    # =========================================================================
    # UTILIDADES
    # =========================================================================
    
    def benchmark(self, size: int = 1024, iterations: int = 10) -> Dict[str, float]:
        """Ejecuta benchmark de GPU."""
        results = {}
        
        # Vector add
        a = np.random.randn(size * size).astype(np.float32)
        b = np.random.randn(size * size).astype(np.float32)
        
        start = time.perf_counter()
        for _ in range(iterations):
            _ = self.vector_add(a, b)
        elapsed = (time.perf_counter() - start) / iterations * 1000
        results["vector_add_ms"] = elapsed
        
        # MatMul
        a = np.random.randn(size, size).astype(np.float32)
        b = np.random.randn(size, size).astype(np.float32)
        
        start = time.perf_counter()
        for _ in range(iterations):
            _ = self.matmul(a, b)
        elapsed = (time.perf_counter() - start) / iterations * 1000
        results["matmul_ms"] = elapsed
        
        return results
    
    def get_metrics(self) -> Dict[str, Any]:
        """Obtiene mÃ©tricas de rendimiento."""
        return {
            "dispatches": self.metrics.dispatches,
            "total_time_ms": self.metrics.total_time_ms,
            "avg_dispatch_ms": self.metrics.total_time_ms / max(1, self.metrics.dispatches),
            "transfers_to_gpu": self.metrics.transfers_to_gpu,
            "transfers_from_gpu": self.metrics.transfers_from_gpu,
            "bytes_transferred": self.metrics.bytes_transferred,
            "buffer_pool": self.buffer_pool.stats(),
        }
    
    def shutdown(self):
        """Cierra el runtime."""
        self.wait()
        self.buffer_pool.clear()
        self.kernel_cache.clear()
        self.pipeline_cache.clear()
        self._executor.shutdown(wait=False)
        print("ðŸ‘‹ GPU Runtime cerrado.")


# =========================================================================
# CLASES DE SINCRONIZACIÃ“N
# =========================================================================

class GPUFence:
    """Fence para sincronizaciÃ³n GPU."""
    
    def __init__(self, gpu: GPU):
        self.gpu = gpu
        self._signaled = False
    
    def wait(self, timeout_ms: float = None):
        """Espera a que el fence sea seÃ±alizado."""
        self.gpu.wait()
        self._signaled = True
    
    def is_signaled(self) -> bool:
        return self._signaled
    
    def reset(self):
        self._signaled = False


class GPUEvent:
    """Evento para sincronizaciÃ³n GPU."""
    
    def __init__(self, gpu: GPU):
        self.gpu = gpu
        self._recorded = False
        self._timestamp = 0.0
    
    def record(self):
        """Graba el evento."""
        self._timestamp = time.perf_counter()
        self._recorded = True
    
    def elapsed_ms(self, other: 'GPUEvent') -> float:
        """Tiempo transcurrido entre dos eventos."""
        return (self._timestamp - other._timestamp) * 1000


class GPUStream:
    """Stream/Queue para operaciones asÃ­ncronas."""
    
    def __init__(self, gpu: GPU):
        self.gpu = gpu
        self._operations: List[Any] = []
    
    def dispatch(self, kernel: GPUKernel, *buffers: GPUBuffer, 
                 groups: Tuple[int, int, int] = (1, 1, 1)):
        """Encola un dispatch."""
        self._operations.append(('dispatch', kernel, buffers, groups))
    
    def synchronize(self):
        """Ejecuta todas las operaciones encoladas."""
        for op in self._operations:
            if op[0] == 'dispatch':
                _, kernel, buffers, groups = op
                self.gpu.dispatch(kernel, *buffers, groups=groups)
        self._operations.clear()
        self.gpu.wait()


# =========================================================================
# DEMO
# =========================================================================

def demo():
    """Demo de FFI GPU."""
    print("\n" + "=" * 60)
    print("   ðŸŽ® Demo FFI GPU")
    print("=" * 60)
    
    gpu = GPU()
    
    # Test vector add
    print("\nðŸ“Š Test Vector Add:")
    a = np.array([1, 2, 3, 4, 5], dtype=np.float32)
    b = np.array([10, 20, 30, 40, 50], dtype=np.float32)
    c = gpu.vector_add(a, b)
    print(f"   {a} + {b} = {c}")
    
    # Test matmul
    print("\nðŸ“Š Test MatMul:")
    A = np.array([[1, 2], [3, 4]], dtype=np.float32)
    B = np.array([[5, 6], [7, 8]], dtype=np.float32)
    C = gpu.matmul(A, B)
    print(f"   A @ B =\n{C}")
    
    # Benchmark
    print("\nâš¡ Benchmark (256x256):")
    results = gpu.benchmark(size=256, iterations=5)
    for name, time_ms in results.items():
        print(f"   {name}: {time_ms:.2f} ms")
    
    # MÃ©tricas
    print("\nðŸ“ˆ MÃ©tricas:")
    metrics = gpu.get_metrics()
    for key, value in metrics.items():
        if not isinstance(value, dict):
            print(f"   {key}: {value}")
    
    gpu.shutdown()
    print("\nâœ… Demo completado")


if __name__ == "__main__":
    demo()
