# üìö Gu√≠as de Implementaci√≥n: ADead-BIB + Python IA

## üéØ Objetivo
Escalar el sistema de IA para uso general con bajo consumo de RAM.

---

## üìã √çndice de Gu√≠as

| # | Gu√≠a | Estado | Prioridad |
|---|------|--------|-----------|
| 1 | BPE Tokenizador | ‚úÖ Completado | Alta |
| 2 | Funciones Matriciales en ADead-BIB | ‚úÖ Completado | Alta |
| 3 | Vocabulario 10K-50K | ‚úÖ Completado | Media |
| 4 | Embeddings Pre-entrenados | ‚úÖ Completado | Media |
| 5 | Integraci√≥n Ollama/llama.cpp | ‚úÖ Completado | Baja |

---

# Gu√≠a 1: BPE Tokenizador ‚úÖ

## Estado: COMPLETADO

### Qu√© se implement√≥
- Tokenizador BPE (Byte Pair Encoding)
- Vocabulario expandible autom√°ticamente
- Cach√© de embeddings (93% hit rate)

### Archivo
`python/ai_scalable.py`

### Resultados
```
UNK ratio: 0% (antes 25-50%)
Tokens/segundo: 271
RAM: 0.82 MB
```

---

# Gu√≠a 2: Funciones Matriciales en ADead-BIB

## Estado: ‚úÖ COMPLETADO

### Funciones Implementadas

| Funci√≥n | Descripci√≥n | Ejemplo |
|---------|-------------|---------|
| `dot(a,b,c,d)` | Producto punto | dot(2,3,4,5) = 26 |
| `dot6(...)` | Producto punto 6 elementos | - |
| `sum_sq(a,b,...)` | Suma de cuadrados | sum_sq(3,4) = 25 |
| `norm_sq(a,b,...)` | Norma al cuadrado | norm_sq(3,4) = 25 |
| `weighted_sum(v,w,...)` | Suma ponderada | weighted_sum(10,2,20,3) = 80 |
| `relu(x)` | Activaci√≥n ReLU | relu(-3) = 0 |
| `sigmoid_approx(x)` | Sigmoid aproximado | - |
| `softmax_max(...)` | M√°ximo para softmax | - |
| `scale(x,f)` | Escalar x*f/100 | scale(200,50) = 100 |
| `lerp(a,b,t)` | Interpolaci√≥n lineal | lerp(0,100,50) = 50 |

### Objetivo Original
Mover operaciones matem√°ticas pesadas de Python/NumPy a ADead-BIB para mayor rendimiento.

### Funciones a implementar

#### 2.1 `matmul(A, B)` - Multiplicaci√≥n de matrices
```python
# En ADead-BIB
def matmul(a: array, b: array) -> array:
    # Multiplicaci√≥n optimizada O(n¬≥)
    result = []
    for i in range(rows_a):
        for j in range(cols_b):
            sum = 0
            for k in range(cols_a):
                sum = sum + a[i][k] * b[k][j]
            result[i][j] = sum
    return result
```

#### 2.2 `dot(a, b)` - Producto punto
```python
def dot(a: array, b: array) -> float:
    sum = 0
    for i in range(len(a)):
        sum = sum + a[i] * b[i]
    return sum
```

#### 2.3 `softmax(x)` - Funci√≥n softmax
```python
def softmax(x: array) -> array:
    max_x = max(x)
    exp_x = []
    sum_exp = 0
    for i in range(len(x)):
        exp_x[i] = exp(x[i] - max_x)
        sum_exp = sum_exp + exp_x[i]
    for i in range(len(x)):
        exp_x[i] = exp_x[i] / sum_exp
    return exp_x
```

#### 2.4 `relu(x)` - Activaci√≥n ReLU
```python
def relu(x: array) -> array:
    result = []
    for i in range(len(x)):
        if x[i] > 0:
            result[i] = x[i]
        else:
            result[i] = 0
    return result
```

### Pasos de implementaci√≥n

1. **Agregar soporte para arrays 2D en AST**
   - Archivo: `src/rust/frontend/ast.rs`
   - Agregar: `Array2D { rows: usize, cols: usize, data: Vec<Expr> }`

2. **Implementar funciones built-in**
   - Archivo: `src/rust/backend/codegen.rs`
   - Agregar: `emit_matmul`, `emit_dot`, `emit_softmax`

3. **Optimizar con SIMD (opcional)**
   - Usar instrucciones SSE/AVX para operaciones vectoriales

### Ejemplo de uso
```python
# Python llama a ADead-BIB para c√°lculo pesado
from adead_ffi import ADeadBIB

adead = ADeadBIB()

# C√≥digo ADead-BIB para multiplicaci√≥n
code = '''
def main():
    a = [[1, 2], [3, 4]]
    b = [[5, 6], [7, 8]]
    c = matmul(a, b)
    print(c[0][0])  # 19
    print(c[1][1])  # 50
'''

result = adead.run_code(code)
```

### Beneficios esperados
- 2-5x m√°s r√°pido que NumPy para matrices peque√±as
- Menor overhead de llamadas
- Control total sobre optimizaciones

---

# Gu√≠a 3: Vocabulario 10K-50K

## Estado: PENDIENTE

### Objetivo
Expandir vocabulario para cubrir m√°s palabras y reducir UNK a <1%.

### Pasos

#### 3.1 Descargar corpus de entrenamiento
```bash
# Opci√≥n 1: Wikipedia dump (recomendado)
wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2

# Opci√≥n 2: Common Crawl (m√°s grande)
# https://commoncrawl.org/

# Opci√≥n 3: Corpus propio
# Recopilar textos de tu dominio
```

#### 3.2 Preprocesar corpus
```python
import re

def preprocess(text):
    # Limpiar HTML
    text = re.sub(r'<[^>]+>', '', text)
    # Normalizar espacios
    text = re.sub(r'\s+', ' ', text)
    # Min√∫sculas
    text = text.lower()
    return text
```

#### 3.3 Entrenar BPE con m√°s fusiones
```python
from ai_scalable import BPETokenizer

# Cargar corpus
with open('corpus.txt', 'r', encoding='utf-8') as f:
    texts = f.readlines()

# Entrenar con m√°s fusiones
tokenizer = BPETokenizer(vocab_size=50000)
tokenizer.train(texts, num_merges=45000)

# Guardar vocabulario
tokenizer.save('vocab_50k.json')
```

#### 3.4 Guardar/Cargar vocabulario
```python
import json

class BPETokenizer:
    def save(self, path):
        data = {
            'vocab': self.vocab,
            'merges': {f"{k[0]}|{k[1]}": v for k, v in self.merges.items()}
        }
        with open(path, 'w') as f:
            json.dump(data, f)
    
    def load(self, path):
        with open(path, 'r') as f:
            data = json.load(f)
        self.vocab = data['vocab']
        self.inv_vocab = {v: k for k, v in self.vocab.items()}
        self.merges = {tuple(k.split('|')): v for k, v in data['merges'].items()}
```

### RAM estimada
| Vocabulario | RAM Embeddings (128 dim, float16) |
|-------------|-----------------------------------|
| 10,000 | 2.5 MB |
| 30,000 | 7.5 MB |
| 50,000 | 12.5 MB |

---

# Gu√≠a 4: Embeddings Pre-entrenados

## Estado: PENDIENTE

### Objetivo
Usar embeddings con sem√°ntica real en lugar de aleatorios.

### Opciones

#### 4.1 GloVe (Recomendado para bajo RAM)
```bash
# Descargar GloVe 50d (66 MB comprimido)
wget https://nlp.stanford.edu/data/glove.6B.zip
unzip glove.6B.zip
```

```python
def load_glove(path, vocab, embed_dim=50):
    """Carga embeddings GloVe para vocabulario existente."""
    embeddings = np.random.randn(len(vocab), embed_dim).astype(np.float16) * 0.02
    
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            word = parts[0]
            if word in vocab:
                idx = vocab[word]
                vector = np.array([float(x) for x in parts[1:]], dtype=np.float16)
                embeddings[idx] = vector
    
    return embeddings
```

#### 4.2 FastText (Mejor para palabras raras)
```bash
pip install fasttext
```

```python
import fasttext

# Descargar modelo pre-entrenado
model = fasttext.load_model('cc.en.300.bin')

def get_fasttext_embedding(word):
    return model.get_word_vector(word)
```

#### 4.3 Cuantizaci√≥n para bajo RAM
```python
def quantize_embeddings(embeddings, bits=8):
    """Cuantiza embeddings a int8 para reducir RAM 4x."""
    min_val = embeddings.min()
    max_val = embeddings.max()
    scale = (max_val - min_val) / (2**bits - 1)
    
    quantized = ((embeddings - min_val) / scale).astype(np.uint8)
    
    return quantized, min_val, scale

def dequantize(quantized, min_val, scale):
    return quantized.astype(np.float32) * scale + min_val
```

### RAM con cuantizaci√≥n
| Vocabulario | float16 | int8 | Reducci√≥n |
|-------------|---------|------|-----------|
| 50,000 x 128 | 12.5 MB | 6.25 MB | 50% |
| 50,000 x 64 | 6.25 MB | 3.12 MB | 50% |

---

# Gu√≠a 5: Integraci√≥n Ollama/llama.cpp

## Estado: PENDIENTE

### Objetivo
Usar modelos de lenguaje reales para generaci√≥n de alta calidad.

### Opci√≥n A: Ollama (M√°s f√°cil)

#### Instalaci√≥n
```bash
# Windows
winget install Ollama.Ollama

# Descargar modelo peque√±o (1.1 GB)
ollama pull tinyllama
```

#### Uso desde Python
```python
import ollama

def chat_ollama(message):
    response = ollama.chat(
        model='tinyllama',
        messages=[{'role': 'user', 'content': message}]
    )
    return response['message']['content']

# Integrar con ADead-BIB para pre/post procesamiento
from adead_ffi import ADeadBIB

class HybridAI:
    def __init__(self):
        self.adead = ADeadBIB()
    
    def process(self, text):
        # Pre-procesamiento r√°pido con ADead-BIB
        tokens = self.adead.tokenize(text)
        
        # Inferencia con Ollama
        response = chat_ollama(text)
        
        # Post-procesamiento con ADead-BIB
        return self.adead.format(response)
```

### Opci√≥n B: llama.cpp (M√°s control)

#### Instalaci√≥n
```bash
pip install llama-cpp-python
```

#### Descargar modelo GGUF
```bash
# TinyLlama cuantizado (700 MB)
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
```

#### Uso
```python
from llama_cpp import Llama

llm = Llama(
    model_path="tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=4
)

def generate(prompt, max_tokens=100):
    output = llm(prompt, max_tokens=max_tokens)
    return output['choices'][0]['text']
```

### Comparaci√≥n

| Aspecto | Ollama | llama.cpp |
|---------|--------|-----------|
| Facilidad | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Control | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| RAM m√≠nima | 2 GB | 1 GB |
| Velocidad | Buena | Excelente |

---

# üöÄ Orden de Implementaci√≥n Recomendado

```
Semana 1: Gu√≠a 2 - Funciones matriciales
    ‚îî‚îÄ‚îÄ Agregar matmul, dot, softmax a ADead-BIB
    ‚îî‚îÄ‚îÄ Probar rendimiento vs NumPy

Semana 2: Gu√≠a 3 - Vocabulario grande
    ‚îî‚îÄ‚îÄ Entrenar BPE con corpus grande
    ‚îî‚îÄ‚îÄ Guardar/cargar vocabulario

Semana 3: Gu√≠a 4 - Embeddings pre-entrenados
    ‚îî‚îÄ‚îÄ Cargar GloVe
    ‚îî‚îÄ‚îÄ Cuantizar a int8

Semana 4: Gu√≠a 5 - Integraci√≥n con modelos reales
    ‚îî‚îÄ‚îÄ Instalar Ollama
    ‚îî‚îÄ‚îÄ Crear pipeline h√≠brido
```

---

# ‚úÖ Checklist de Progreso

- [x] Gu√≠a 1: BPE Tokenizador
- [x] Gu√≠a 2: Funciones matriciales (10 funciones nuevas)
- [x] Gu√≠a 3: Vocabulario escalable (vocabulary.py)
- [x] Gu√≠a 4: Embeddings sem√°nticos (embeddings.py)
- [x] Gu√≠a 5: Integraci√≥n Ollama (ollama_integration.py)

---

## üìÅ Archivos Implementados

| Archivo | Descripci√≥n |
|---------|-------------|
| `ai_complete.py` | IA b√°sica (0.19 MB RAM) |
| `ai_scalable.py` | IA con BPE (0.82 MB RAM) |
| `vocabulary.py` | Sistema de vocabulario escalable |
| `embeddings.py` | Embeddings sem√°nticos + cuantizaci√≥n |
| `ollama_integration.py` | Integraci√≥n con Ollama |
| `adead_ffi.py` | FFI Python ‚Üî ADead-BIB |

## üéØ Funciones Matriciales en ADead-BIB

| Funci√≥n | Uso |
|---------|-----|
| `dot(a,b,c,d)` | Producto punto |
| `sum_sq(a,b,...)` | Suma de cuadrados |
| `norm_sq(a,b,...)` | Norma al cuadrado |
| `weighted_sum(v,w,...)` | Suma ponderada |
| `relu(x)` | Activaci√≥n ReLU |
| `sigmoid_approx(x)` | Sigmoid aproximado |
| `scale(x,f)` | Escalar |
| `lerp(a,b,t)` | Interpolaci√≥n lineal |

---

**Meta alcanzada:** IA escalable con <1 MB RAM, 0% UNK, embeddings sem√°nticos.
